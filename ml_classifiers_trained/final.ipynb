{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T08:50:28.902698Z",
     "iopub.status.busy": "2025-01-30T08:50:28.902235Z",
     "iopub.status.idle": "2025-01-30T08:50:32.357722Z",
     "shell.execute_reply": "2025-01-30T08:50:32.356684Z",
     "shell.execute_reply.started": "2025-01-30T08:50:28.902661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 librosa==0.10.2.post1 tqdm==4.67.1 pandas==2.2.2 joblib==1.4.2 scikit-learn==1.6.1 tensorflow==2.17.1 python_speech_features==0.6 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from python_speech_features import logfbank, fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Feature Extraction ------------------\n",
    "def pad_or_truncate(features, target_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates the feature matrix to ensure uniform length.\n",
    "    \n",
    "    Parameters:\n",
    "    - features (np.ndarray): Feature matrix of shape (Time, Features).\n",
    "    - target_length (int): Desired number of time frames.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: Padded or truncated feature matrix.\n",
    "    \"\"\"\n",
    "    if features.shape[0] < target_length:\n",
    "        padding = target_length - features.shape[0]\n",
    "        padded_features = np.pad(features, ((0, padding), (0, 0)), mode='constant')\n",
    "        return padded_features\n",
    "    else:\n",
    "        return features[:target_length, :]\n",
    "\n",
    "def extract_filterbank_energies(audio, samplerate=44100, nfilt=40):\n",
    "    \"\"\"\n",
    "    Extracts Filterbank Energies from the audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: Filterbank energy features.\n",
    "    \"\"\"\n",
    "    features, _ = fbank(audio, samplerate, nfilt=nfilt)\n",
    "    return features\n",
    "\n",
    "def extract_log_filterbank_energies(audio, samplerate=44100, nfilt=40):\n",
    "    \"\"\"\n",
    "    Extracts Log Filterbank Energies from the audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: Log Filterbank energy features.\n",
    "    \"\"\"\n",
    "    features = logfbank(audio, samplerate, nfilt=nfilt)  # Remove unpacking\n",
    "    return features\n",
    "\n",
    "def extract_spectral_subband_centroids(audio, samplerate=44100, nfilt=40):\n",
    "    \"\"\"\n",
    "    Extracts Spectral Subband Centroids from the audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: Spectral Subband Centroids features.\n",
    "    \"\"\"\n",
    "    filter_banks, energies = fbank(audio, samplerate, nfilt=nfilt)\n",
    "    centroids = np.zeros(filter_banks.shape)\n",
    "    for i in range(filter_banks.shape[0]):\n",
    "        if np.sum(filter_banks[i]) != 0:\n",
    "            centroids[i] = np.sum(filter_banks[i] * np.arange(1, nfilt + 1)) / np.sum(filter_banks[i])\n",
    "        else:\n",
    "            centroids[i] = 0\n",
    "    return centroids\n",
    "\n",
    "def extract_spncc(audio, samplerate=44100, nfilt=40, ncep=13):\n",
    "    \"\"\"\n",
    "    Extracts Power-Normalized Cepstral Coefficients (SPNCC) from the audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "    - ncep (int): Number of cepstral coefficients.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: SPNCC features.\n",
    "    \"\"\"\n",
    "    filter_banks = extract_filterbank_energies(audio, samplerate, nfilt)\n",
    "    power = np.sum(filter_banks, axis=1)\n",
    "    power_normalized = filter_banks / (power[:, np.newaxis] + 1e-10)\n",
    "    spncc = librosa.feature.mfcc(S=np.log(power_normalized + 1e-10), n_mfcc=ncep).T\n",
    "    return spncc\n",
    "\n",
    "def extract_msrcc(audio, samplerate=44100, nfilt=40, ncep=13):\n",
    "    \"\"\"\n",
    "    Extracts Magnitude-based Spectral Root Cepstral Coefficients (MSRCC) from the audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "    - ncep (int): Number of cepstral coefficients.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: MSRCC features.\n",
    "    \"\"\"\n",
    "    filter_banks = extract_filterbank_energies(audio, samplerate, nfilt)\n",
    "    root_power_spectrum = np.sqrt(filter_banks + 1e-10)\n",
    "    msrcc = librosa.feature.mfcc(S=np.log(root_power_spectrum + 1e-10), n_mfcc=ncep).T\n",
    "    return msrcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Dataset Loader ------------------\n",
    "class DysarthriaDataset:\n",
    "    def __init__(self, data_path, severity_mapping, sr=44100, target_length=128, feature_type='logfbank', n_mfcc=13, nfilt=40):\n",
    "        \"\"\"\n",
    "        Initializes the dataset loader.\n",
    "        \n",
    "        Parameters:\n",
    "        - data_path (str): Path to the dataset directory.\n",
    "        - severity_mapping (dict): Mapping of severity levels to speaker IDs.\n",
    "        - sr (int): Sampling rate for audio files.\n",
    "        - target_length (int): Target length for feature matrices.\n",
    "        - feature_type (str): Type of feature to extract ('fbank', 'logfbank', 'spncc', 'msrcc', 'subband_centroid').\n",
    "        - n_mfcc (int): Number of MFCC coefficients to extract (for SPNCC and MSRCC).\n",
    "        - nfilt (int): Number of filterbanks.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.severity_mapping = severity_mapping\n",
    "        self.sr = sr\n",
    "        self.target_length = target_length\n",
    "        self.feature_type = feature_type\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.nfilt = nfilt\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        severity_to_label = {severity: i for i, severity in enumerate(severity_mapping.keys())}\n",
    "        \n",
    "        for severity, speakers in severity_mapping.items():\n",
    "            for speaker in speakers:\n",
    "                speaker_path = os.path.join(data_path, speaker)\n",
    "                if not os.path.exists(speaker_path):\n",
    "                    print(f\"Warning: Speaker path {speaker_path} does not exist. Skipping.\")\n",
    "                    continue\n",
    "                for file in os.listdir(speaker_path):\n",
    "                    if file.endswith(\".wav\"):\n",
    "                        self.data.append(os.path.join(speaker_path, file))\n",
    "                        self.labels.append(severity_to_label[severity])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the feature vector and label for a given index.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx (int): Index of the sample.\n",
    "        \n",
    "        Returns:\n",
    "        - tuple: (feature_vector (np.ndarray), label (int))\n",
    "        \"\"\"\n",
    "        file_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        audio, _ = librosa.load(file_path, sr=self.sr)\n",
    "        \n",
    "        # Extract features based on feature_type\n",
    "        if self.feature_type == 'fbank':\n",
    "            features = extract_filterbank_energies(audio, self.sr, self.nfilt)\n",
    "        elif self.feature_type == 'logfbank':\n",
    "            features = extract_log_filterbank_energies(audio, self.sr, self.nfilt)\n",
    "        elif self.feature_type == 'subband_centroid':\n",
    "            features = extract_spectral_subband_centroids(audio, self.sr, self.nfilt)\n",
    "        elif self.feature_type == 'spncc':\n",
    "            features = extract_spncc(audio, self.sr, self.nfilt, self.n_mfcc)\n",
    "        elif self.feature_type == 'msrcc':\n",
    "            features = extract_msrcc(audio, self.sr, self.nfilt, self.n_mfcc)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported feature type: {self.feature_type}\")\n",
    "        \n",
    "        features_fixed = pad_or_truncate(features, self.target_length)\n",
    "        \n",
    "        # Aggregate features\n",
    "        if self.feature_type in ['fbank', 'logfbank', 'subband_centroid', 'spncc', 'msrcc']:\n",
    "            # Calculate mean and std across time frames\n",
    "            feature_mean = features_fixed.mean(axis=0)\n",
    "            feature_std = features_fixed.std(axis=0)\n",
    "            # Concatenate mean and std\n",
    "            feature_vector = np.concatenate([feature_mean, feature_std])\n",
    "        else:\n",
    "            feature_vector = features_fixed.flatten()\n",
    "        \n",
    "        return feature_vector, label\n",
    "    \n",
    "    def get_all_features_labels(self):\n",
    "        \"\"\"\n",
    "        Extracts features and labels for the entire dataset.\n",
    "        \n",
    "        Returns:\n",
    "        - tuple: (X (np.ndarray), y (np.ndarray))\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        for idx in tqdm(range(len(self)), desc=f\"Extracting {self.feature_type} Features\"):\n",
    "            features, label = self[idx]\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "# ------------------ Artificial Neural Network (ANN) with TensorFlow ------------------\n",
    "def create_ann_model(input_dim, hidden_layers=[64, 32], dropout_rate=0.5, activation='relu', optimizer='adam'):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow Keras ANN model with mixed precision and early stopping.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_dim (int): Number of input features.\n",
    "    - hidden_layers (list): List containing the number of neurons in each hidden layer.\n",
    "    - dropout_rate (float): Dropout rate for regularization.\n",
    "    - activation (str): Activation function for hidden layers.\n",
    "    - optimizer (str): Optimizer for training.\n",
    "    \n",
    "    Returns:\n",
    "    - tf.keras.Model: Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers[0], input_dim=input_dim, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for neurons in hidden_layers[1:]:\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(4, activation='softmax', dtype='float32'))  # Ensure output layer is float32\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Main Pipeline ------------------\n",
    "def main():\n",
    "    # ------------------ Configuration ------------------\n",
    "    # Define severity mapping\n",
    "    severity_mapping = {\n",
    "        \"HIGH\": [\"M01\", \"M04\", \"M12\", \"F03\"],\n",
    "        \"MEDIUM\": [\"F02\", \"M07\", \"M16\"],\n",
    "        \"LOW\": [\"F04\", \"M05\", \"M11\"],\n",
    "        \"VERY LOW\": [\"F05\", \"M08\", \"M09\", \"M10\", \"M14\"]\n",
    "    }\n",
    "\n",
    "    dataset_path = \"/kaggle/input/dysarthria-data/noisereduced-uaspeech\"  # Update this path as needed\n",
    "\n",
    "    # Define the list of MFCCs to evaluate\n",
    "    mfcc_list = [13, 26, 39, 52]\n",
    "\n",
    "    # Define target length for feature matrices\n",
    "    target_length = 128\n",
    "\n",
    "    # Define random state for reproducibility\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "    # Define the path for the CSV file\n",
    "    results_csv_path = \"classifier_results.csv\"\n",
    "\n",
    "    # Initialize or load existing results\n",
    "    if os.path.exists(results_csv_path):\n",
    "        print(f\"Loading existing results from {results_csv_path}...\")\n",
    "        results_df = pd.read_csv(results_csv_path)\n",
    "        processed_classifiers = results_df['Classifier'].tolist()\n",
    "    else:\n",
    "        print(\"Initializing a new results DataFrame.\")\n",
    "        results_df = pd.DataFrame(columns=[\"MFCCs\", \"Classifier\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "        processed_classifiers = []\n",
    "\n",
    "    # Iterate over different numbers of MFCCs\n",
    "    for n_mfcc in mfcc_list:\n",
    "        print(f\"\\n=== Processing with {n_mfcc} MFCCs ===\")\n",
    "        \n",
    "        # Define list of feature types to evaluate separately\n",
    "        feature_types = ['msrcc', 'fbank', 'logfbank', 'subband_centroid', 'spncc']\n",
    "        \n",
    "        for feature_type in feature_types:\n",
    "            print(f\"\\n--- Extracting and Evaluating Feature: {feature_type} ---\")\n",
    "            \n",
    "            # Initialize dataset with current feature type and MFCC\n",
    "            dataset = DysarthriaDataset(\n",
    "                data_path=dataset_path,\n",
    "                severity_mapping=severity_mapping,\n",
    "                sr=44100,\n",
    "                target_length=target_length,\n",
    "                feature_type=feature_type,\n",
    "                n_mfcc=n_mfcc,\n",
    "                nfilt=40\n",
    "            )\n",
    "\n",
    "            # Extract features and labels\n",
    "            X, y = dataset.get_all_features_labels()\n",
    "            \n",
    "            print(f\"Feature matrix shape: {X.shape}\")\n",
    "            print(f\"Labels distribution: {np.bincount(y)}\")\n",
    "\n",
    "            # ------------------ Dataset Splitting ------------------\n",
    "            # Split data into Train (70%), Validation (15%), Test (15%)\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    "            )\n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_STATE\n",
    "            )\n",
    "\n",
    "            print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "            print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "            print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "            # ------------------ Classifier Definitions and Hyperparameter Grids ------------------\n",
    "            # Define classifiers and their hyperparameter grids\n",
    "            classifiers = {\n",
    "                \"Logistic Regression\": {\n",
    "                    \"model\": LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "                    \"params\": {\n",
    "                        \"classifier__C\": [0.1, 1, 10, 100],\n",
    "                        \"classifier__penalty\": ['l2'],\n",
    "                        \"classifier__solver\": ['lbfgs']\n",
    "                    }\n",
    "                },\n",
    "                \"Support Vector Machine\": {\n",
    "                    \"model\": SVC(probability=True, random_state=RANDOM_STATE),\n",
    "                    \"params\": {\n",
    "                        \"classifier__C\": [0.1, 1, 10],\n",
    "                        \"classifier__gamma\": ['scale', 'auto'],\n",
    "                        \"classifier__kernel\": ['rbf', 'poly']\n",
    "                    }\n",
    "                },\n",
    "                \"k-Nearest Neighbors\": {\n",
    "                    \"model\": KNeighborsClassifier(),\n",
    "                    \"params\": {\n",
    "                        \"classifier__n_neighbors\": [3, 5, 7],\n",
    "                        \"classifier__weights\": ['uniform', 'distance'],\n",
    "                        \"classifier__metric\": ['euclidean', 'manhattan']\n",
    "                    }\n",
    "                },\n",
    "                \"Decision Tree\": {\n",
    "                    \"model\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "                    \"params\": {\n",
    "                        \"classifier__max_depth\": [None, 10, 20, 30],\n",
    "                        \"classifier__min_samples_split\": [2, 5, 10],\n",
    "                        \"classifier__criterion\": ['gini', 'entropy']\n",
    "                    }\n",
    "                },\n",
    "                \"Random Forest\": {\n",
    "                    \"model\": RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "                    \"params\": {\n",
    "                        \"classifier__n_estimators\": [100, 200],\n",
    "                        \"classifier__max_depth\": [None, 10, 20],\n",
    "                        \"classifier__min_samples_split\": [2, 5],\n",
    "                        \"classifier__criterion\": ['gini', 'entropy']\n",
    "                    }\n",
    "                },\n",
    "                \"Gradient Boosting\": {\n",
    "                    \"model\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "                    \"params\": {\n",
    "                        \"classifier__n_estimators\": [100, 200],\n",
    "                        \"classifier__learning_rate\": [0.01, 0.1],\n",
    "                        \"classifier__max_depth\": [3, 5],\n",
    "                        \"classifier__subsample\": [0.8, 1.0]\n",
    "                    }\n",
    "                },\n",
    "                \"Naive Bayes\": {\n",
    "                    \"model\": GaussianNB(),\n",
    "                    \"params\": {\n",
    "                        \"classifier__var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "                    }\n",
    "                },\n",
    "                \"Artificial Neural Network (ANN)\": {\n",
    "                     \"model\": KerasClassifierWithCallbacks(build_fn=create_ann_model, verbose=0),\n",
    "                     \"params\": {\n",
    "                         \"classifier__hidden_layers\": [[64, 32], [128, 64, 32]],\n",
    "                         \"classifier__dropout_rate\": [0.3, 0.5],\n",
    "                         \"classifier__activation\": ['relu', 'tanh'],\n",
    "                         \"classifier__optimizer\": ['adam', 'rmsprop'],\n",
    "                         \"classifier__batch_size\": [32, 64],\n",
    "                         \"classifier__epochs\": [50, 100],\n",
    "                         \"classifier__callbacks\": [[early_stopping]]\n",
    "                     }\n",
    "                 }\n",
    "            }\n",
    "\n",
    "            # ------------------ Training, Evaluation, and Result Storage ------------------\n",
    "            for clf_name, clf_info in classifiers.items():\n",
    "                # Skip already processed classifiers for current feature and MFCC setting\n",
    "                if ((results_df['MFCCs'] == n_mfcc) & \n",
    "                    (results_df['Classifier'] == clf_name) &\n",
    "                    (results_df['Feature_Type'] == feature_type)).any():\n",
    "                    print(f\"Skipping {clf_name} with {feature_type} and {n_mfcc} MFCCs as it has already been processed.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"\\n--- Training {clf_name} with {feature_type} and {n_mfcc} MFCCs ---\")\n",
    "\n",
    "                # Create pipeline\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('classifier', clf_info['model'])\n",
    "                ])\n",
    "\n",
    "                # Define GridSearchCV\n",
    "                grid = GridSearchCV(\n",
    "                    estimator=pipeline,\n",
    "                    param_grid=clf_info['params'],\n",
    "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Fit GridSearchCV\n",
    "                grid.fit(X_train, y_train)\n",
    "\n",
    "                # Best estimator\n",
    "                best_estimator = grid.best_estimator_\n",
    "                best_params = grid.best_params_\n",
    "                best_score = grid.best_score_\n",
    "\n",
    "                print(f\"Best Parameters for {clf_name}: {best_params}\")\n",
    "                print(f\"Best Cross-Validation Accuracy: {best_score * 100:.2f}%\")\n",
    "\n",
    "                # Evaluate on Test Set\n",
    "                y_pred = best_estimator.predict(X_test)\n",
    "                test_accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "                test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n",
    "                    y_test, y_pred, average='weighted'\n",
    "                )\n",
    "\n",
    "                print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "                print(f\"Test Precision: {test_precision * 100:.2f}%\")\n",
    "                print(f\"Test Recall: {test_recall * 100:.2f}%\")\n",
    "                print(f\"Test F1-Score: {test_f1 * 100:.2f}%\")\n",
    "\n",
    "                # Append results to DataFrame\n",
    "                new_row = {\n",
    "                    \"MFCCs\": n_mfcc,\n",
    "                    \"Feature_Type\": feature_type,\n",
    "                    \"Classifier\": clf_name,\n",
    "                    \"Accuracy\": f\"{test_accuracy:.2f}%\",\n",
    "                    \"Precision\": f\"{test_precision * 100:.2f}%\",\n",
    "                    \"Recall\": f\"{test_recall * 100:.2f}%\",\n",
    "                    \"F1-Score\": f\"{test_f1 * 100:.2f}%\"\n",
    "                }\n",
    "                results_df = results_df.append(new_row, ignore_index=True)\n",
    "\n",
    "                # Save results to CSV\n",
    "                results_df.to_csv(results_csv_path, index=False)\n",
    "                print(f\"Results for {clf_name} with {feature_type} and {n_mfcc} MFCCs saved to {results_csv_path}.\")\n",
    "\n",
    "                # Optionally, save the best model for each classifier and feature setting\n",
    "                model_save_path = f\"models/{clf_name.replace(' ', '_')}_{feature_type}_mfcc_{n_mfcc}.joblib\"\n",
    "                os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "                joblib.dump(best_estimator, model_save_path)\n",
    "                print(f\"Model saved to {model_save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6531226,
     "sourceId": 10556447,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
