{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:33:34.413934Z",
     "iopub.status.busy": "2025-01-31T07:33:34.413636Z",
     "iopub.status.idle": "2025-01-31T07:33:40.510083Z",
     "shell.execute_reply": "2025-01-31T07:33:40.509002Z",
     "shell.execute_reply.started": "2025-01-31T07:33:34.413912Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4 librosa==0.10.2.post1 tqdm==4.67.1 pandas==2.2.2 joblib==1.4.2 scikit-learn==1.5.0 tensorflow==2.17.1 python_speech_features==0.6 scikeras==0.10.0 matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:33:48.194990Z",
     "iopub.status.busy": "2025-01-31T07:33:48.194661Z",
     "iopub.status.idle": "2025-01-31T07:34:01.701172Z",
     "shell.execute_reply": "2025-01-31T07:34:01.700443Z",
     "shell.execute_reply.started": "2025-01-31T07:33:48.194941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Import Libraries ------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from python_speech_features import logfbank, fbank\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:34:07.853780Z",
     "iopub.status.busy": "2025-01-31T07:34:07.853212Z",
     "iopub.status.idle": "2025-01-31T07:34:07.864372Z",
     "shell.execute_reply": "2025-01-31T07:34:07.863528Z",
     "shell.execute_reply.started": "2025-01-31T07:34:07.853752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Feature Extraction Functions ------------------\n",
    "def pad_or_truncate(features, target_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates the feature matrix to ensure uniform length.\n",
    "\n",
    "    Parameters:\n",
    "    - features (np.ndarray): Feature matrix of shape (Time, Features).\n",
    "    - target_length (int): Desired number of time frames.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Padded or truncated feature matrix.\n",
    "    \"\"\"\n",
    "    if features.shape[0] < target_length:\n",
    "        padding = target_length - features.shape[0]\n",
    "        padded_features = np.pad(features, ((0, padding), (0, 0)), mode='constant')\n",
    "        return padded_features\n",
    "    else:\n",
    "        return features[:target_length, :]\n",
    "\n",
    "def extract_filterbank_energies(audio, samplerate=44100, nfilt=40):\n",
    "    \"\"\"\n",
    "    Extracts Filterbank Energies from the audio signal.\n",
    "\n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Filterbank energy features.\n",
    "    \"\"\"\n",
    "    features, _ = fbank(audio, samplerate, nfilt=nfilt)\n",
    "    return features\n",
    "\n",
    "def extract_log_filterbank_energies(audio, samplerate=44100, nfilt=40):\n",
    "    \"\"\"\n",
    "    Extracts Log Filterbank Energies from the audio signal.\n",
    "\n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Log Filterbank energy features.\n",
    "    \"\"\"\n",
    "    features = logfbank(audio, samplerate, nfilt=nfilt)  # Remove unpacking\n",
    "    return features\n",
    "\n",
    "def extract_spectral_subband_centroids(audio, samplerate=44100, nfilt=40):\n",
    "    \"\"\"\n",
    "    Extracts Spectral Subband Centroids from the audio signal.\n",
    "\n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Spectral Subband Centroids features.\n",
    "    \"\"\"\n",
    "    filter_banks, _ = fbank(audio, samplerate, nfilt=nfilt)\n",
    "    centroids = np.zeros(filter_banks.shape)\n",
    "    for i in range(filter_banks.shape[0]):\n",
    "        if np.sum(filter_banks[i]) != 0:\n",
    "            centroids[i] = np.sum(filter_banks[i] * np.arange(1, nfilt + 1)) / np.sum(filter_banks[i])\n",
    "        else:\n",
    "            centroids[i] = 0\n",
    "    return centroids\n",
    "\n",
    "def extract_spncc(audio, samplerate=44100, nfilt=40, ncep=13):\n",
    "    \"\"\"\n",
    "    Extracts Power-Normalized Cepstral Coefficients (SPNCC) from the audio signal.\n",
    "\n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "    - ncep (int): Number of cepstral coefficients.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: SPNCC features.\n",
    "    \"\"\"\n",
    "    filter_banks = extract_filterbank_energies(audio, samplerate, nfilt)\n",
    "    power = np.sum(filter_banks, axis=1)\n",
    "    power_normalized = filter_banks / (power[:, np.newaxis] + 1e-10)\n",
    "    spncc = librosa.feature.mfcc(S=np.log(power_normalized + 1e-10), n_mfcc=ncep).T\n",
    "    return spncc\n",
    "\n",
    "def extract_msrcc(audio, samplerate=44100, nfilt=40, ncep=13):\n",
    "    \"\"\"\n",
    "    Extracts Magnitude-based Spectral Root Cepstral Coefficients (MSRCC) from the audio signal.\n",
    "\n",
    "    Parameters:\n",
    "    - audio (np.ndarray): Audio time series.\n",
    "    - samplerate (int): Sampling rate.\n",
    "    - nfilt (int): Number of filterbanks.\n",
    "    - ncep (int): Number of cepstral coefficients.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: MSRCC features.\n",
    "    \"\"\"\n",
    "    filter_banks = extract_filterbank_energies(audio, samplerate, nfilt)\n",
    "    root_power_spectrum = np.sqrt(filter_banks + 1e-10)\n",
    "    msrcc = librosa.feature.mfcc(S=np.log(root_power_spectrum + 1e-10), n_mfcc=ncep).T\n",
    "    return msrcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:34:20.414865Z",
     "iopub.status.busy": "2025-01-31T07:34:20.414582Z",
     "iopub.status.idle": "2025-01-31T07:34:20.605602Z",
     "shell.execute_reply": "2025-01-31T07:34:20.604707Z",
     "shell.execute_reply.started": "2025-01-31T07:34:20.414842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Dataset Loader Class ------------------\n",
    "class DysarthriaDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        severity_mapping,\n",
    "        sr=44100,\n",
    "        target_length=128,\n",
    "        feature_type='logfbank',\n",
    "        n_mfcc=13,\n",
    "        nfilt=40\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the dataset loader.\n",
    "\n",
    "        Parameters:\n",
    "        - data_path (str): Path to the dataset directory.\n",
    "        - severity_mapping (dict): Mapping of severity levels to integer labels.\n",
    "        - sr (int): Sampling rate for audio files.\n",
    "        - target_length (int): Target length for feature matrices.\n",
    "        - feature_type (str): Type of feature to extract ('fbank', 'logfbank', 'spncc', 'msrcc', 'subband_centroid').\n",
    "        - n_mfcc (int): Number of MFCC coefficients to extract (for SPNCC and MSRCC).\n",
    "        - nfilt (int): Number of filterbanks.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.severity_mapping = severity_mapping\n",
    "        self.sr = sr\n",
    "        self.target_length = target_length\n",
    "        self.feature_type = feature_type\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.nfilt = nfilt\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for severity, label in severity_mapping.items():\n",
    "            folder_path = os.path.join(data_path, severity)\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"Warning: Folder path {folder_path} does not exist. Skipping.\")\n",
    "                continue\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.lower().endswith((\".wav\", \".mp3\", \".flac\")):\n",
    "                    self.data.append(os.path.join(folder_path, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the feature vector and label for a given index.\n",
    "\n",
    "        Parameters:\n",
    "        - idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: (feature_vector (np.ndarray), label (int))\n",
    "        \"\"\"\n",
    "        file_path = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        audio, _ = librosa.load(file_path, sr=self.sr)\n",
    "\n",
    "        # Extract features based on feature_type\n",
    "        if self.feature_type == 'fbank':\n",
    "            features = extract_filterbank_energies(audio, self.sr, self.nfilt)\n",
    "        elif self.feature_type == 'logfbank':\n",
    "            features = extract_log_filterbank_energies(audio, self.sr, self.nfilt)\n",
    "        elif self.feature_type == 'subband_centroid':\n",
    "            features = extract_spectral_subband_centroids(audio, self.sr, self.nfilt)\n",
    "        elif self.feature_type == 'spncc':\n",
    "            features = extract_spncc(audio, self.sr, self.nfilt, self.n_mfcc)\n",
    "        elif self.feature_type == 'msrcc':\n",
    "            features = extract_msrcc(audio, self.sr, self.nfilt, self.n_mfcc)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported feature type: {self.feature_type}\")\n",
    "\n",
    "        features_fixed = pad_or_truncate(features, self.target_length)\n",
    "\n",
    "        # Aggregate features\n",
    "        if self.feature_type in ['fbank', 'logfbank', 'subband_centroid', 'spncc', 'msrcc']:\n",
    "            # Calculate mean and std across time frames\n",
    "            feature_mean = features_fixed.mean(axis=0)\n",
    "            feature_std = features_fixed.std(axis=0)\n",
    "            # Concatenate mean and std\n",
    "            feature_vector = np.concatenate([feature_mean, feature_std])\n",
    "        else:\n",
    "            feature_vector = features_fixed.flatten()\n",
    "\n",
    "        return feature_vector, label\n",
    "\n",
    "    def get_all_features_labels(self):\n",
    "        \"\"\"\n",
    "        Extracts features and labels for the entire dataset.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: (X (np.ndarray), y (np.ndarray))\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        for idx in tqdm(range(len(self)), desc=f\"Extracting {self.feature_type} Features\"):\n",
    "            features, label = self[idx]\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "        return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:34:27.380527Z",
     "iopub.status.busy": "2025-01-31T07:34:27.380240Z",
     "iopub.status.idle": "2025-01-31T07:34:27.397553Z",
     "shell.execute_reply": "2025-01-31T07:34:27.396744Z",
     "shell.execute_reply.started": "2025-01-31T07:34:27.380505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ ANN Model Creation Function ------------------\n",
    "def create_ann_model(\n",
    "    input_dim,\n",
    "    hidden_layers=[64, 32],\n",
    "    dropout_rate=0.5,\n",
    "    activation='relu',\n",
    "    optimizer='adam'\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow Keras ANN model with early stopping.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dim (int): Number of input features.\n",
    "    - hidden_layers (list): List containing the number of neurons in each hidden layer.\n",
    "    - dropout_rate (float): Dropout rate for regularization.\n",
    "    - activation (str): Activation function for hidden layers.\n",
    "    - optimizer (str): Optimizer for training.\n",
    "\n",
    "    Returns:\n",
    "    - tf.keras.Model: Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers[0], input_dim=input_dim, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for neurons in hidden_layers[1:]:\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax', dtype='float32'))  # Ensure output layer is float32\n",
    "\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ------------------ Visualization Functions ------------------\n",
    "def plot_training_history(history, clf_name, feature_type, n_mfcc, save_dir='plots/ann'):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss and accuracy over epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - history (History): Keras History object.\n",
    "    - clf_name (str): Name of the classifier.\n",
    "    - feature_type (str): Type of feature used.\n",
    "    - n_mfcc (int): Number of MFCCs.\n",
    "    - save_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Loss Curve for {clf_name} ({feature_type}, MFCC={n_mfcc})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(save_dir, f\"{clf_name}_{feature_type}_mfcc{n_mfcc}_loss.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy Curve for {clf_name} ({feature_type}, MFCC={n_mfcc})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    acc_plot_path = os.path.join(save_dir, f\"{clf_name}_{feature_type}_mfcc{n_mfcc}_accuracy.png\")\n",
    "    plt.savefig(acc_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_classification_report(y_true, y_pred, clf_name, feature_type, n_mfcc, save_dir='plots/classification_reports'):\n",
    "    \"\"\"\n",
    "    Plots the classification report as a heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True labels.\n",
    "    - y_pred (np.ndarray): Predicted labels.\n",
    "    - clf_name (str): Name of the classifier.\n",
    "    - feature_type (str): Type of feature used.\n",
    "    - n_mfcc (int): Number of MFCCs.\n",
    "    - save_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(report_df.iloc[:-3, :].T, annot=True, cmap='Blues')\n",
    "    plt.title(f'Classification Report for {clf_name} ({feature_type}, MFCC={n_mfcc})')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Classes')\n",
    "    report_plot_path = os.path.join(save_dir, f\"{clf_name}_{feature_type}_mfcc{n_mfcc}_classification_report.png\")\n",
    "    plt.savefig(report_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix_matrix(y_true, y_pred, clf_name, feature_type, n_mfcc, save_dir='plots/confusion_matrices'):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix as a heatmap.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True labels.\n",
    "    - y_pred (np.ndarray): Predicted labels.\n",
    "    - clf_name (str): Name of the classifier.\n",
    "    - feature_type (str): Type of feature used.\n",
    "    - n_mfcc (int): Number of MFCCs.\n",
    "    - save_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
    "    plt.title(f'Confusion Matrix for {clf_name} ({feature_type}, MFCC={n_mfcc})')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    cm_plot_path = os.path.join(save_dir, f\"{clf_name}_{feature_type}_mfcc{n_mfcc}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_results_overview(results_df, metric='Accuracy', save_dir='plots/overview'):\n",
    "    \"\"\"\n",
    "    Plots an overview of a specific metric across different classifiers and MFCCs.\n",
    "\n",
    "    Parameters:\n",
    "    - results_df (pd.DataFrame): DataFrame containing the results.\n",
    "    - metric (str): The metric to plot ('Accuracy', 'Precision', 'Recall', 'F1-Score').\n",
    "    - save_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=results_df, x='Classifier', y=metric, hue='MFCCs')\n",
    "    plt.title(f'{metric} Comparison Across Classifiers and MFCCs')\n",
    "    plt.xlabel('Classifier')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='MFCCs')\n",
    "    plt.tight_layout()\n",
    "    overview_plot_path = os.path.join(save_dir, f\"overview_{metric.lower()}.png\")\n",
    "    plt.savefig(overview_plot_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve_multiclass(y_true, y_score, clf_name, feature_type, n_mfcc, save_dir='plots/roc_curves'):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve for multi-class classifiers using a one-vs-rest approach.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (np.ndarray): True labels.\n",
    "    - y_score (np.ndarray): Predicted scores or probabilities.\n",
    "    - clf_name (str): Name of the classifier.\n",
    "    - feature_type (str): Type of feature used.\n",
    "    - n_mfcc (int): Number of MFCCs.\n",
    "    - save_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    classes = np.unique(y_true)\n",
    "    y_true_binarized = label_binarize(y_true, classes=classes)\n",
    "    n_classes = y_true_binarized.shape[1]\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_binarized.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves for {clf_name} ({feature_type}, MFCC={n_mfcc})')\n",
    "    plt.legend(loc='lower right')\n",
    "    roc_plot_path = os.path.join(save_dir, f\"{clf_name}_{feature_type}_mfcc{n_mfcc}_roc_curve.png\")\n",
    "    plt.savefig(roc_plot_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-31T07:34:36.954577Z",
     "iopub.status.busy": "2025-01-31T07:34:36.954250Z",
     "iopub.status.idle": "2025-01-31T07:34:36.974981Z",
     "shell.execute_reply": "2025-01-31T07:34:36.974165Z",
     "shell.execute_reply.started": "2025-01-31T07:34:36.954548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ Main Pipeline Function ------------------\n",
    "def main_pipeline():\n",
    "    # ------------------ Configuration ------------------\n",
    "    # Define severity mapping (Folder Name to Label)\n",
    "    severity_mapping = {\n",
    "        \"VERY LOW\": 0,\n",
    "        \"LOW\": 1,\n",
    "        \"MEDIUM\": 2,\n",
    "        \"HIGH\": 3\n",
    "    }\n",
    "\n",
    "    # Update this path to point to your dataset directory\n",
    "    dataset_path = \"/kaggle/input/class-data/class-data\"  # Example: \"/kaggle/input/dysarthria-data/noisereduced-uaspeech\"\n",
    "\n",
    "    # Define the list of MFCCs to evaluate\n",
    "    mfcc_list = [13, 26, 39, 52]\n",
    "\n",
    "    # Define target length for feature matrices\n",
    "    target_length = 128\n",
    "\n",
    "    # Define random state for reproducibility\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "    # Define the path for the CSV file\n",
    "    results_csv_path = \"classifier_results.csv\"\n",
    "\n",
    "    # Initialize or load existing results\n",
    "    if os.path.exists(results_csv_path):\n",
    "        print(f\"Loading existing results from {results_csv_path}...\")\n",
    "        results_df = pd.read_csv(results_csv_path)\n",
    "    else:\n",
    "        print(\"Initializing a new results DataFrame.\")\n",
    "        results_df = pd.DataFrame(columns=[\"MFCCs\", \"Feature_Type\", \"Classifier\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "    # Iterate over different numbers of MFCCs\n",
    "    for n_mfcc in mfcc_list:\n",
    "        print(f\"\\n=== Processing with {n_mfcc} MFCCs ===\")\n",
    "\n",
    "        # Define list of feature types to evaluate separately\n",
    "        feature_types = ['msrcc', 'fbank', 'logfbank', 'subband_centroid', 'spncc']\n",
    "\n",
    "        for feature_type in feature_types:\n",
    "            print(f\"\\n--- Extracting and Evaluating Feature: {feature_type} ---\")\n",
    "\n",
    "            # Initialize dataset with current feature type and MFCC\n",
    "            dataset = DysarthriaDataset(\n",
    "                data_path=dataset_path,\n",
    "                severity_mapping=severity_mapping,\n",
    "                sr=44100,\n",
    "                target_length=target_length,\n",
    "                feature_type=feature_type,\n",
    "                n_mfcc=n_mfcc,\n",
    "                nfilt=40\n",
    "            )\n",
    "\n",
    "            # Extract features and labels\n",
    "            X, y = dataset.get_all_features_labels()\n",
    "\n",
    "            print(f\"Feature matrix shape: {X.shape}\")\n",
    "            print(f\"Labels distribution: {np.bincount(y)}\")\n",
    "\n",
    "            # ------------------ Dataset Splitting ------------------\n",
    "            # Split data into Train (70%), Validation (15%), Test (15%)\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    "            )\n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_STATE\n",
    "            )\n",
    "\n",
    "            print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "            print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "            print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "            # ------------------ Classifier Definitions and Hyperparameter Grids ------------------\n",
    "            # Define classifiers and their hyperparameter grids\n",
    "            classifiers = {\n",
    "                \"Logistic Regression\": {\n",
    "                    \"model\": LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "                    \"params\": {\n",
    "                        \"classifier__C\": [0.1, 1, 10, 100],\n",
    "                        \"classifier__penalty\": ['l2'],\n",
    "                        \"classifier__solver\": ['lbfgs']\n",
    "                    }\n",
    "                },\n",
    "                \"Support Vector Machine\": {\n",
    "                    \"model\": SVC(probability=True, random_state=RANDOM_STATE),\n",
    "                    \"params\": {\n",
    "                        \"classifier__C\": [0.1, 1, 10],\n",
    "                        \"classifier__gamma\": ['scale', 'auto'],\n",
    "                        \"classifier__kernel\": ['rbf', 'poly']\n",
    "                    }\n",
    "                },\n",
    "                \"k-Nearest Neighbors\": {\n",
    "                    \"model\": KNeighborsClassifier(),\n",
    "                    \"params\": {\n",
    "                        \"classifier__n_neighbors\": [3, 5, 7],\n",
    "                        \"classifier__weights\": ['uniform', 'distance'],\n",
    "                        \"classifier__metric\": ['euclidean', 'manhattan']\n",
    "                    }\n",
    "                },\n",
    "                \"Decision Tree\": {\n",
    "                    \"model\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "                    \"params\": {\n",
    "                        \"classifier__max_depth\": [None, 10, 20, 30],\n",
    "                        \"classifier__min_samples_split\": [2, 5, 10],\n",
    "                        \"classifier__criterion\": ['gini', 'entropy']\n",
    "                    }\n",
    "                },\n",
    "                \"Random Forest\": {\n",
    "                    \"model\": RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "                    \"params\": {\n",
    "                        \"classifier__n_estimators\": [100, 200],\n",
    "                        \"classifier__max_depth\": [None, 10, 20],\n",
    "                        \"classifier__min_samples_split\": [2, 5],\n",
    "                        \"classifier__criterion\": ['gini', 'entropy']\n",
    "                    }\n",
    "                },\n",
    "                \"Gradient Boosting\": {\n",
    "                    \"model\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "                    \"params\": {\n",
    "                        \"classifier__n_estimators\": [100, 200],\n",
    "                        \"classifier__learning_rate\": [0.01, 0.1],\n",
    "                        \"classifier__max_depth\": [3, 5],\n",
    "                        \"classifier__subsample\": [0.8, 1.0]\n",
    "                    }\n",
    "                },\n",
    "                \"Naive Bayes\": {\n",
    "                    \"model\": GaussianNB(),\n",
    "                    \"params\": {\n",
    "                        \"classifier__var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "                    }\n",
    "                },\n",
    "                \"Artificial Neural Network (ANN)\": {\n",
    "                    \"model\": KerasClassifier(\n",
    "                        model=create_ann_model,\n",
    "                        input_dim=X_train.shape[1],\n",
    "                        hidden_layers=[64, 32],\n",
    "                        dropout_rate=0.5,\n",
    "                        activation='relu',\n",
    "                        optimizer='adam',\n",
    "                        epochs=100,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "                        verbose=0\n",
    "                    ),\n",
    "                    \"params\": {\n",
    "                        \"classifier__hidden_layers\": [[64, 32], [128, 64, 32]],\n",
    "                        \"classifier__dropout_rate\": [0.3, 0.5],\n",
    "                        \"classifier__activation\": ['relu', 'tanh'],\n",
    "                        \"classifier__optimizer\": ['adam', 'rmsprop'],\n",
    "                        \"classifier__batch_size\": [32, 64],\n",
    "                        \"classifier__epochs\": [50, 100],\n",
    "                        \"classifier__callbacks\": [[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)]]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # ------------------ Training, Evaluation, and Result Storage ------------------\n",
    "            for clf_name, clf_info in classifiers.items():\n",
    "                # Skip already processed classifiers for current feature and MFCC setting\n",
    "                if (\n",
    "                    ((results_df['MFCCs'] == n_mfcc) & \n",
    "                    (results_df['Feature_Type'] == feature_type) &\n",
    "                    (results_df['Classifier'] == clf_name)).any()\n",
    "                ):\n",
    "                    print(f\"Skipping {clf_name} with {feature_type} and {n_mfcc} MFCCs as it has already been processed.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"\\n--- Training {clf_name} with {feature_type} and {n_mfcc} MFCCs ---\")\n",
    "\n",
    "                # Create pipeline\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('classifier', clf_info['model'])\n",
    "                ])\n",
    "\n",
    "                # Define GridSearchCV\n",
    "                grid = GridSearchCV(\n",
    "                    estimator=pipeline,\n",
    "                    param_grid=clf_info['params'],\n",
    "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Fit GridSearchCV\n",
    "                grid.fit(X_train, y_train)\n",
    "\n",
    "                # Best estimator\n",
    "                best_estimator = grid.best_estimator_\n",
    "                best_params = grid.best_params_\n",
    "                best_score = grid.best_score_\n",
    "\n",
    "                print(f\"Best Parameters for {clf_name}: {best_params}\")\n",
    "                print(f\"Best Cross-Validation Accuracy: {best_score * 100:.2f}%\")\n",
    "\n",
    "                # Evaluate on Test Set\n",
    "                y_pred = best_estimator.predict(X_test)\n",
    "                test_accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "                test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n",
    "                    y_test, y_pred, average='weighted'\n",
    "                )\n",
    "\n",
    "                print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "                print(f\"Test Precision: {test_precision * 100:.2f}%\")\n",
    "                print(f\"Test Recall: {test_recall * 100:.2f}%\")\n",
    "                print(f\"Test F1-Score: {test_f1 * 100:.2f}%\")\n",
    "\n",
    "                # Append results to DataFrame\n",
    "                new_row = pd.DataFrame([{\n",
    "                    \"MFCCs\": n_mfcc,\n",
    "                    \"Feature_Type\": feature_type,\n",
    "                    \"Classifier\": clf_name,\n",
    "                    \"Accuracy\": f\"{test_accuracy:.2f}%\",\n",
    "                    \"Precision\": f\"{test_precision * 100:.2f}%\",\n",
    "                    \"Recall\": f\"{test_recall * 100:.2f}%\",\n",
    "                    \"F1-Score\": f\"{test_f1 * 100:.2f}%\"\n",
    "                }])\n",
    "                results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "                # Save results to CSV\n",
    "                results_df.to_csv(results_csv_path, index=False)\n",
    "                print(f\"Results for {clf_name} with {feature_type} and {n_mfcc} MFCCs saved to {results_csv_path}.\")\n",
    "\n",
    "                # ------------------ Visualization ------------------\n",
    "                # Plot classification report\n",
    "                plot_classification_report(y_test, y_pred, clf_name, feature_type, n_mfcc)\n",
    "\n",
    "                # Plot confusion matrix\n",
    "                plot_confusion_matrix_matrix(y_test, y_pred, clf_name, feature_type, n_mfcc)\n",
    "\n",
    "                # If the classifier is ANN, plot training history\n",
    "                if clf_name == \"Artificial Neural Network (ANN)\":\n",
    "                    # Extract the history from scikeras\n",
    "                    if hasattr(best_estimator.named_steps['classifier'], 'model_') and hasattr(best_estimator.named_steps['classifier'].model_, 'history'):\n",
    "                        history = best_estimator.named_steps['classifier'].model_.history\n",
    "                        plot_training_history(history, clf_name, feature_type, n_mfcc)\n",
    "                    else:\n",
    "                        print(\"No training history available for ANN.\")\n",
    "\n",
    "                # Plot ROC Curve if the classifier provides probability estimates\n",
    "                if hasattr(best_estimator.named_steps['classifier'], \"predict_proba\"):\n",
    "                    y_score = best_estimator.predict_proba(X_test)\n",
    "                    plot_roc_curve_multiclass(y_test, y_score, clf_name, feature_type, n_mfcc)\n",
    "                elif hasattr(best_estimator.named_steps['classifier'], \"decision_function\"):\n",
    "                    # For classifiers like SVM without predict_proba\n",
    "                    y_score = best_estimator.decision_function(X_test)\n",
    "                    # If y_score is 1D, reshape it\n",
    "                    if len(y_score.shape) == 1:\n",
    "                        y_score = np.vstack([-y_score, y_score]).T\n",
    "                    plot_roc_curve_multiclass(y_test, y_score, clf_name, feature_type, n_mfcc)\n",
    "                else:\n",
    "                    print(f\"Classifier {clf_name} does not support probability estimates for ROC curve.\")\n",
    "\n",
    "                # ------------------ Save the Best Model ------------------\n",
    "                model_save_path = f\"models/{clf_name.replace(' ', '')}_{feature_type}_mfcc{n_mfcc}.joblib\"\n",
    "                os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "                joblib.dump(best_estimator, model_save_path)\n",
    "                print(f\"Model saved to {model_save_path}.\")\n",
    "\n",
    "    # ------------------ Aggregated Results Visualization Function ------------------\n",
    "    def aggregate_and_plot_results(results_csv_path):\n",
    "        \"\"\"\n",
    "        Aggregates results from the CSV and plots overview metrics.\n",
    "\n",
    "        Parameters:\n",
    "        - results_csv_path (str): Path to the results CSV file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(results_csv_path):\n",
    "            print(f\"Results CSV file {results_csv_path} does not exist.\")\n",
    "            return\n",
    "\n",
    "        results_df = pd.read_csv(results_csv_path)\n",
    "\n",
    "        # Convert percentage strings to float\n",
    "        for metric in [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]:\n",
    "            results_df[metric] = results_df[metric].str.rstrip('%').astype(float)\n",
    "\n",
    "        # Plot Accuracy Overview\n",
    "        plot_results_overview(results_df, metric='Accuracy')\n",
    "\n",
    "        # Plot Precision Overview\n",
    "        plot_results_overview(results_df, metric='Precision')\n",
    "\n",
    "        # Plot Recall Overview\n",
    "        plot_results_overview(results_df, metric='Recall')\n",
    "\n",
    "        # Plot F1-Score Overview\n",
    "        plot_results_overview(results_df, metric='F1-Score')\n",
    "\n",
    "        print(\"\\nAggregated result visualizations have been saved.\")\n",
    "\n",
    "    # ------------------ Execute Main Pipeline ------------------\n",
    "    main_pipeline()\n",
    "\n",
    "    # ------------------ Execute Aggregated Plots ------------------\n",
    "    aggregate_and_plot_results(\"classifier_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-31T07:38:34.238Z",
     "iopub.execute_input": "2025-01-31T07:34:42.849664Z",
     "iopub.status.busy": "2025-01-31T07:34:42.849357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new results DataFrame.\n",
      "\n",
      "=== Processing with 13 MFCCs ===\n",
      "\n",
      "--- Extracting and Evaluating Feature: msrcc ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting msrcc Features: 100%|██████████| 11437/11437 [03:32<00:00, 53.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (11437, 26)\n",
      "Labels distribution: [3825 2281 2295 3036]\n",
      "Train set: 8005 samples\n",
      "Validation set: 1716 samples\n",
      "Test set: 1716 samples\n",
      "\n",
      "--- Training Logistic Regression with msrcc and 13 MFCCs ---\n",
      "Best Parameters for Logistic Regression: {'classifier__C': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
      "Best Cross-Validation Accuracy: 54.85%\n",
      "Test Accuracy: 55.83%\n",
      "Test Precision: 54.51%\n",
      "Test Recall: 55.83%\n",
      "Test F1-Score: 53.59%\n",
      "Results for Logistic Regression with msrcc and 13 MFCCs saved to classifier_results.csv.\n",
      "Model saved to models/LogisticRegression_msrcc_mfcc13.joblib.\n",
      "\n",
      "--- Training Support Vector Machine with msrcc and 13 MFCCs ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Execute Script ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6578217,
     "sourceId": 10624357,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
